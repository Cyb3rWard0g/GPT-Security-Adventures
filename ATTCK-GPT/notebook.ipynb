{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATT&CK GPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ATT&CK Groups Knowledge Base"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules and Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attackcti import attack_client\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.getLogger('taxii2client').setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few variables\n",
    "current_directory = os.path.dirname(\"__file__\")\n",
    "knowledge_directory = os.path.join(current_directory, \"knowledge\")\n",
    "db_directory = os.path.join(current_directory, \"db\")\n",
    "templates_directory = os.path.join(current_directory, \"templates\")\n",
    "group_template = os.path.join(templates_directory, \"group.md\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize ATT&CK Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift = attack_client()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ATT&CK Groups Knowledge\n",
    "Gettings technique STIX objects used by all groups accross all ATT&CK matrices.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techniques_used_by_groups = lift.get_techniques_used_by_all_groups()\n",
    "techniques_used_by_groups[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ATT&CK Groups Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from jinja2 import Template\n",
    "\n",
    "# Create Group docs\n",
    "all_groups = dict()\n",
    "for technique in techniques_used_by_groups:\n",
    "    if technique['id'] not in all_groups:\n",
    "        group = dict()\n",
    "        group['group_name'] = technique['name']\n",
    "        group['group_id'] = technique['external_references'][0]['external_id']\n",
    "        group['created'] = technique['created']\n",
    "        group['modified'] = technique['modified']\n",
    "        group['description'] = technique['description']\n",
    "        group['aliases'] = technique['aliases']\n",
    "        if 'x_mitre_contributors' in technique:\n",
    "            group['contributors'] = technique['x_mitre_contributors']\n",
    "        group['techniques'] = []\n",
    "        all_groups[technique['id']] = group\n",
    "    technique_used = dict()\n",
    "    technique_used['matrix'] = technique['matrix']\n",
    "    technique_used['domain'] = technique['x_mitre_domains']\n",
    "    technique_used['platform'] = technique['platform']\n",
    "    technique_used['tactics'] = technique['tactic']\n",
    "    technique_used['technique_id'] = technique['technique_id']\n",
    "    technique_used['technique_name'] = technique['technique']\n",
    "    technique_used['use'] = technique['relationship_description']\n",
    "    if 'data_sources' in technique:\n",
    "        technique_used['data_sources'] = technique['data_sources']\n",
    "    all_groups[technique['id']]['techniques'].append(technique_used)\n",
    "\n",
    "print(\"[+] Creating markadown files for each group..\")\n",
    "markdown_template = Template(open(group_template).read())\n",
    "for key in list(all_groups.keys()):\n",
    "    group = all_groups[key]\n",
    "    print(\"  [>>] Creating markdown file for {}..\".format(group['group_name']))\n",
    "    group_for_render = copy.deepcopy(group)\n",
    "    markdown = markdown_template.render(metadata=group_for_render, group_name=group['group_name'], group_id=group['group_id'])\n",
    "    file_name = (group['group_name']).replace(' ','_')\n",
    "    open(f'{knowledge_directory}/{file_name}.md', encoding='utf-8', mode='w').write(markdown)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Knowledge Base Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "group_files = glob.glob(os.path.join(knowledge_directory, \"*.md\"))\n",
    "\n",
    "# Loading Markdown files\n",
    "md_docs = []\n",
    "print(\"[+] Loading Group markdown files..\")\n",
    "for group in group_files:\n",
    "    print(f' [*] Loading {os.path.basename(group)}')\n",
    "    loader = UnstructuredMarkdownLoader(group)\n",
    "    md_docs.extend(loader.load())\n",
    "\n",
    "print(f'[+] Number of .md documents processed: {len(md_docs)}')\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "token_counts = [tiktoken_len(doc.page_content) for doc in md_docs]\n",
    "\n",
    "print(f\"\"\"[+] Token Counts:\n",
    "Min: {min(token_counts)}\n",
    "Avg: {int(sum(token_counts) / len(token_counts))}\n",
    "Max: {max(token_counts)}\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from tqdm.auto import tqdm\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking Text\n",
    "print('[+] Initializing RecursiveCharacterTextSplitter..')\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,  # number of tokens overlap between chunks\n",
    "    length_function=tiktoken_len,\n",
    "    separators=['\\n\\n', '\\n', ' ', '']\n",
    ")\n",
    "\n",
    "print('[+] Splitting documents in chunks..')\n",
    "chunks = text_splitter.split_documents(md_docs)\n",
    "print(type(chunks))\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[+] Splitting text in chunks..')\n",
    "json_documents = []\n",
    "chunks_documents = []\n",
    "m = hashlib.md5()\n",
    "for doc in tqdm(md_docs):\n",
    "    doc_name = os.path.basename(doc.metadata['source'])\n",
    "    m.update(doc_name.encode('utf-8'))\n",
    "    uid = m.hexdigest()[:12]\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Add JSON object to array\n",
    "        json_documents.append({\n",
    "        'id': f'{uid}-{i}',\n",
    "        'text': chunk,\n",
    "        'source': doc_name\n",
    "        })\n",
    "        # Create docs\n",
    "        document = Document(page_content=chunk, metadata={\"source\": doc_name})\n",
    "        chunks_documents.append(document)\n",
    "\n",
    "print(f'[+] Final Documents count: {len(json_documents)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Knowledge Base as JSONL File (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[+] Exporting groups as .jsonl file..')\n",
    "with open(f'{os.path.join(db_directory, \"attack-groups.jsonl\")}', 'w') as f:\n",
    "    for doc in json_documents:\n",
    "        f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import openai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your key: https://platform.openai.com/account/api-keys\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[+] Starting embedding..\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Send text chunks to OpenAI Embeddings API\n",
    "print(\"[+] Sending chunks to OpenAI Embeddings API..\")\n",
    "db = FAISS.from_documents(chunks_documents, embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Database Pickle File (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "current_directory = os.path.dirname(\"__file__\")\n",
    "db_directory = os.path.join(current_directory, \"db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file = os.path.join(db_directory,\"db.pkl\")\n",
    "\n",
    "if os.path.exists(db_file):\n",
    "    with open(db_file, \"rb\") as f:\n",
    "        db = pickle.load(f)\n",
    "else:\n",
    "    # Save vectorstore\n",
    "    print(\"[+] Create Pickle file..\")\n",
    "    with open(db_file, \"wb\") as f:\n",
    "        pickle.dump(db, f)\n",
    "\n",
    "print(type(db))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query ATT&CK Groups Knowledge Base"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize OpenAI (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "# Get your key: https://platform.openai.com/account/api-keys\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Vector Store Retriever\n",
    "The retriever interface is a generic interface that makes it easy to combine documents with language models. This interface exposes a get_relevant_documents method which takes in a query (a string) and returns a list of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\":10})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Relevant Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are some phishing techniques used by threat actors?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Getting relevant documents for query..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='APT39 leveraged spearphishing emails with malicious attachments to initially compromise victims.(Citation: FireEye APT39 Jan 2019)(Citation: Symantec Chafer February 2018)(Citation: FBI FLASH APT39 September 2020)|', metadata={'source': 'APT39.md'}),\n",
       " Document(page_content='Lazarus Group has been observed targeting organizations using spearphishing documents with embedded malicious payloads. (Citation: Novetta Threat Research Group February 2016) Highly targeted spear phishing campaigns have been conducted against a U.S. electric grid company. (Citation: Eduard Kovacs March 2018)|', metadata={'source': 'Lazarus_Group.md'}),\n",
       " Document(page_content='TA505 has used spearphishing emails with malicious attachments to initially compromise victims.(Citation: Proofpoint TA505 Sep 2017)(Citation: Proofpoint TA505 June 2018)(Citation: Proofpoint TA505 Jan 2019)(Citation: Cybereason TA505 April 2019)(Citation: ProofPoint SettingContent-ms July 2018)(Citation: Proofpoint TA505 Mar 2018)(Citation: Trend Micro TA505 June 2019)(Citation: Proofpoint TA505 October 2019)(Citation: IBM TA505 April 2020)|', metadata={'source': 'TA505.md'}),\n",
       " Document(page_content='Nomadic Octopus has targeted victims with spearphishing emails containing malicious attachments.(Citation: Security Affairs DustSquad Oct 2018)(Citation: ESET Nomadic Octopus 2018)|', metadata={'source': 'Nomadic_Octopus.md'}),\n",
       " Document(page_content='APT29 has used spearphishing with a link to trick victims into clicking on a link to a zip file containing malicious files.(Citation: Mandiant No Easy Breach)(Citation: MSTIC NOBELIUM May 2021)(Citation: Secureworks IRON RITUAL USAID Phish May 2021)|\\n|mitre-attack|enterprise-attack|Linux,Windows,macOS|T1203|Exploitation for Client Execution|\\n\\nAPT29 has used multiple software exploits for common client software, like Microsoft Word, Exchange, and Adobe Reader, to gain code execution.(Citation: F-Secure The Dukes)(Citation: Cybersecurity Advisory SVR TTP May 2021)(Citation: MSTIC NOBELIUM May 2021)|\\n|mitre-attack|enterprise-attack|Linux,macOS,Windows,Network|T1090.003|Multi-hop Proxy|A backdoor used by\\n\\nAPT29 created a\\n\\nTor hidden service to forward traffic from the\\n\\nTor client to local ports 3389 (RDP), 139 (Netbios), and 445 (SMB) enabling full remote access from outside the network and has also used TOR.(Citation: Mandiant No Easy Breach)(Citation: MSTIC Nobelium Oct 2021)|\\n|mitre-attack|enterprise-attack|Linux,macOS,Windows|T1090.004|Domain Fronting|\\n\\nAPT29 has used the meek domain fronting plugin for\\n\\nTor to hide the destination of C2 traffic.(Citation: Mandiant No Easy Breach)|\\n|mitre-attack|enterprise-attack|macOS,Windows,Linux|T1027.002|Software Packing|\\n\\nAPT29 used UPX to pack files.(Citation: Mandiant No Easy Breach)|\\n|mitre-attack|enterprise-attack|Windows|T1550.003|Pass the Ticket|\\n\\nAPT29 used Kerberos ticket attacks for lateral movement.(Citation: Mandiant No Easy Breach)|\\n|mitre-attack|enterprise-attack|Windows|T1047|Windows Management Instrumentation|', metadata={'source': 'APT29.md'}),\n",
       " Document(page_content=\"APT33 utilize backdoors capable of capturing screenshots once installed on a system. (Citation: Jacqueline O'Leary et al. September 2017)(Citation: Junnosuke Yagi March 2017)|\\n|mitre-attack|enterprise-attack,ics-attack|Engineering Workstation,Human-Machine Interface,Control Server,Data Historian|T0865|Spearphishing Attachment|\\n\\nAPT33 sent spear phishing emails containing links to HTML application files, which were embedded with malicious code. (Citation: Jacqueline O'Leary et al. September 2017)\\n\\nAPT33 has conducted targeted spear phishing campaigns against U.S. government agencies and private sector companies. (Citation: Andy Greenburg June 2019)|\", metadata={'source': 'APT33.md'}),\n",
       " Document(page_content=\"Techniques Used\\n\\nAPT29 has used residential proxies, including Azure Virtual Machines, to obfuscate their access to victim environments.(Citation: Mandiant APT29 Microsoft 365 2022)|\\n|mitre-attack|enterprise-attack|Office 365,Windows,Google Workspace|T1114.002|Remote Email Collection|\\n\\nAPT29 has collected emails from targeted mailboxes within a compromised Azure AD tenant.(Citation: Mandiant APT29 Microsoft 365 2022)|\\n|mitre-attack|enterprise-attack|Windows,Office 365,Google Workspace|T1098.002|Additional Email Delegate Permissions|\\n\\nAPT29 has used a compromised global administrator account in Azure AD to backdoor a service principal with\\n\\nAPT29 has gained access to a global administrator account in Azure AD.(Citation: Mandiant APT29 Microsoft 365 2022)|\\n|mitre-attack|enterprise-attack|Windows,Azure AD,Office 365,SaaS,IaaS,Linux,macOS,Google Workspace,Containers,Network|T1078|Valid Accounts|\\n\\nAPT29 has used a compromised account to access an organization's VPN infrastructure.(Citation: Mandiant APT29 Microsoft 365 2022)|\\n|mitre-attack|enterprise-attack|Azure AD,Windows,SaaS|T1098.005|Device Registration|\\n\\nAPT29 has enrolled a device in MFA to an Azure AD environment following a successful password guessing attack against a dormant account.(Citation: Mandiant APT29 Microsoft 365 2022)|\\n|mitre-attack|enterprise-attack|Windows,Azure AD,Office 365,SaaS,IaaS,Linux,macOS,Google Workspace,Containers,Network|T1110.001|Password Guessing|\\n\\nAPT29 has successfully conducted password guessing attacks against a list of mailboxes.(Citation: Mandiant APT29 Microsoft 365 2022)|\\n|mitre-attack|enterprise-attack|Windows,macOS,Linux,Containers,IaaS|T1562.001|Disable or Modify Tools|\", metadata={'source': 'APT29.md'}),\n",
       " Document(page_content='APT29 has used various forms of spearphishing attempting to get a user to click on a malicous link.(Citation: MSTIC NOBELIUM May 2021)(Citation: Secureworks IRON RITUAL USAID Phish May 2021)|\\n|mitre-attack|enterprise-attack|Linux,macOS,Windows|T1105|Ingress Tool Transfer|\\n\\nAPT29 has downloaded additional tools and malware onto compromised networks.(Citation: Mandiant No Easy Breach)(Citation: PWC WellMess July 2020)(Citation: F-Secure The Dukes)|\\n|mitre-attack|enterprise-attack|PRE|T1587.001|Malware|\\n\\nAPT29 has used unique malware in many of their operations.(Citation: F-Secure The Dukes)(Citation: Mandiant No Easy Breach)(Citation: MSTIC Nobelium Toolset May 2021)|\\n|mitre-attack|enterprise-attack|Linux,macOS,Windows,Containers|T1036.005|Match Legitimate Name or Location|\\n\\nAPT29 has renamed malicious DLLs with legitimate names to appear benign; they have also created an Azure AD certificate with a Common Name that matched the display name of the compromised service principal.(Citation: SentinelOne NobleBaron June 2021)(Citation: Mandiant APT29 Microsoft 365 2022)|\\n|mitre-attack|enterprise-attack|Windows,Linux,Containers,macOS|T1133|External Remote Services|\\n\\nAPT29 has used compromised identities to access networks via VPNs and Citrix.(Citation: NCSC APT29 July 2020)(Citation: Mandiant APT29 Microsoft 365 2022)|\\n|mitre-attack|enterprise-attack|PRE|T1587.003|Digital Certificates|\\n\\nAPT29 has created self-signed digital certificates to enable mutual TLS authentication for malware.(Citation: PWC WellMess July 2020)(Citation: PWC WellMess C2 August 2020)|\\n|mitre-attack|enterprise-attack|PRE|T1583.006|Web Services|\\n\\nAPT29 has registered algorithmically generated Twitter handles that are used for C2 by malware, such as\\n\\nHAMMERTOSS.', metadata={'source': 'APT29.md'}),\n",
       " Document(page_content='Magic Hound has attempted to lure victims into opening malicious email attachments.(Citation: ClearSky Kittens Back 3 August 2020)|\\n|mitre-attack|enterprise-attack|PRE|T1585.001|Social Media Accounts|\\n\\nMagic Hound has created fake LinkedIn and other social media accounts to contact targets and convince them--through messages and voice communications--to open malicious links.(Citation: ClearSky Kittens Back 3 August 2020)|\\n|mitre-attack|enterprise-attack|PRE|T1584.001|Domains|\\n\\nMagic Hound has used compromised domains to host links targeted to specific phishing victims.(Citation: ClearSky Kittens Back 3 August 2020)(Citation: Proofpoint TA453 July2021)(Citation: Certfa Charming Kitten January 2021)(Citation: Google Iran Threats October 2021)|\\n|mitre-attack|enterprise-attack|PRE|T1583.001|Domains|\\n\\nMagic Hound has registered fraudulent domains such as \"mail-newyorker.com\" and \"news12.com.recover-session-service.site\" to target specific victims with phishing attacks.(Citation: Certfa Charming Kitten January 2021)|\\n|mitre-attack|enterprise-attack|PRE|T1598.003|Spearphishing Link|\\n\\nMagic Hound has used SMS and email messages with links designed to steal credentials or track victims.(Citation: Certfa Charming Kitten January 2021)(Citation: ClearSky Kittens Back 3 August 2020)(Citation: Proofpoint TA453 March 2021)(Citation: Proofpoint TA453 July2021)(Citation: Google Iran Threats October 2021)(Citation: Microsoft Iranian Threat Actor Trends November 2021)|\\n|mitre-attack|enterprise-attack|Windows,Office 365,Google Workspace,macOS,Linux|T1114|Email Collection|\\n\\nMagic Hound has compromised email credentials in order to steal sensitive data.(Citation: Certfa Charming Kitten January 2021)|\\n|mitre-attack|enterprise-attack|PRE|T1586.002|Email Accounts|', metadata={'source': 'Magic_Hound.md'}),\n",
       " Document(page_content='TA551 has used spoofed company emails that were acquired from email clients on previously infected hosts to target other individuals.(Citation: Unit 42 TA551 Jan 2021)|\\n|mitre-attack|enterprise-attack|Linux,macOS,Windows|T1105|Ingress Tool Transfer|\\n\\nTA551 has retrieved DLLs and installer binaries for malware execution from C2.(Citation: Unit 42 TA551 Jan 2021)|\\n|mitre-attack|enterprise-attack|Linux,macOS,Windows|T1027.003|Steganography|\\n\\nTA551 has hidden encoded data for malware DLLs in a PNG.(Citation: Unit 42 TA551 Jan 2021)|\\n|mitre-attack|enterprise-attack|Linux,macOS,Windows|T1071.001|Web Protocols|\\n\\nTA551 has used HTTP for C2 communications.(Citation: Unit 42 Valak July 2020)|\\n|mitre-attack|enterprise-attack|Windows|T1218.011|Rundll32|\\n\\nTA551 has used rundll32.exe to load malicious DLLs.(Citation: Unit 42 TA551 Jan 2021)|\\n|mitre-attack|enterprise-attack|Windows|T1218.010|Regsvr32|\\n\\nTA551 has used regsvr32.exe to load malicious DLLs.(Citation: Unit 42 Valak July 2020)|\\n|mitre-attack|enterprise-attack|Linux,macOS,Windows|T1132.001|Standard Encoding|\\n\\nTA551 has used encoded ASCII text for initial C2 communications.(Citation: Unit 42 Valak July 2020)|\\n|mitre-attack|enterprise-attack|Linux,macOS,Windows,Containers|T1036|Masquerading|\\n\\nTA551 has masked malware DLLs as dat and jpg files.(Citation: Unit 42 TA551 Jan 2021)|\\n|mitre-attack|enterprise-attack|Linux,macOS,Windows|T1027.010|Command Obfuscation|\\n\\nTA551 has used obfuscated variable names in a JavaScript configuration file.(Citation: Unit 42 Valak July 2020)|', metadata={'source': 'TA551.md'})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"[+] Getting relevant documents for query..\")\n",
    "relevant_docs = retriever.get_relevant_documents(query)\n",
    "relevant_docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Threat actors have used spearphishing emails with malicious attachments, links to HTML application files embedded with malicious code, links to malicious zip files, and messages with links designed to steal credentials or track victims.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
    "chain.run(input_documents=relevant_docs, question=query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
